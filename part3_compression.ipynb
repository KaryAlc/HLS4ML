{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Compresión\n",
    "Se importan las bibliotecas necesarias, como TensorFlow, sklearn y numpy. Además, se establece la semilla para la reproducibilidad de los resultados y se configura la variable de entorno 'PATH' para el entorno de Vivado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 01:57:39.766576: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 01:57:39.884846: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML\n",
    "Este código carga el conjunto de datos de etiquetado de jets desde archivos `.npy` y asigna las matrices de características y etiquetas, así como las clases objetivo, a las variables correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de modelo\n",
    "Se construye un modelo de red neuronal con tres capas ocultas, cada una con activación ReLU, seguidas de una capa de salida con activación softmax. Las capas ocultas tienen 64, 32 y 32 neuronas respectivamente, mientras que la capa de salida tiene 2 neuronas para clasificar en cinco clases diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from callbacks import all_callbacks\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(60, 60)),       # Definición de la forma de entrada (imágenes 60x60)\n",
    "    Flatten(),                   # Aplana las imágenes de 2D a 1D\n",
    "    Dense(128, activation='relu'),  # Capa densa con 128 neuronas y activación ReLU\n",
    "    Dropout(0.5),                # Dropout para evitar el sobreajuste\n",
    "    Dense(2, activation='softmax')  # Capa de salida con 2 neuronas para clasificación en 2 clases\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento disperso\n",
    "Este código utiliza la optimización de modelos de TensorFlow para entrenar un modelo disperso (forzando que muchos pesos sean '0'). En este caso, la esparseidad objetivo es del 75%. Se utiliza la función `prune_low_magnitude` para aplicar la poda de magnitud baja al modelo con los parámetros de poda proporcionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrena el modelo\n",
    "Utilizaremos la misma configuración que el modelo de la parte 1: el optimizador Adam con pérdida de entropía cruzada categórica. Los callbacks reducirán la tasa de aprendizaje y guardarán el modelo en un directorio 'model_2'. El modelo no es muy complejo, así que esto debería llevar solo unos minutos incluso en la CPU. Si has reiniciado el kernel del cuaderno después de entrenar una vez, establece train = False para cargar el modelo entrenado en lugar de entrenar nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8561 - accuracy: 0.4583\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.02820, saving model to model_2/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.02820, saving model to model_2/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 1: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 1: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8561 - accuracy: 0.4583 - val_loss: 1.0282 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4175 - accuracy: 0.5417\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 2: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 2: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.4175 - accuracy: 0.5417 - val_loss: 3.2781 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5619 - accuracy: 0.5417\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 3: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 3: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 2.5619 - accuracy: 0.5417 - val_loss: 1.1615 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6159 - accuracy: 0.4167\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 4: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 4: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.6159 - accuracy: 0.4167 - val_loss: 1.7648 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6443 - accuracy: 0.4583\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 5: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 5: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 1.6443 - accuracy: 0.4583 - val_loss: 2.0624 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5463 - accuracy: 0.4583\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.02820\n",
      "\n",
      "Epoch 6: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 6: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 1.5463 - accuracy: 0.4583 - val_loss: 1.1880 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.7917\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 7: val_loss improved from 1.02820 to 0.68320, saving model to model_2/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.02820 to 0.68320, saving model to model_2/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 7: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 7: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.5447 - accuracy: 0.7917 - val_loss: 0.6832 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9917 - accuracy: 0.6667\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 8: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 8: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.9917 - accuracy: 0.6667 - val_loss: 1.0550 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7804 - accuracy: 0.5833\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 9: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 9: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.7804 - accuracy: 0.5833 - val_loss: 1.0591 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.6250\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 10: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 10: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 10: saving model to model_2/KERAS_check_model_epoch10.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.7091 - accuracy: 0.6250 - val_loss: 0.8102 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.7083\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 11: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 11: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.5144 - accuracy: 0.7083 - val_loss: 0.7101 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.7917\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 12: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 12: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4430 - accuracy: 0.7917 - val_loss: 0.8036 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8090 - accuracy: 0.6250\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 13: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 13: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.8090 - accuracy: 0.6250 - val_loss: 0.8267 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.6667\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 14: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 14: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.6610 - accuracy: 0.6667 - val_loss: 0.7680 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.7083\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 15: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 15: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.5210 - accuracy: 0.7083 - val_loss: 0.7246 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6772 - accuracy: 0.7083\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 16: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 16: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.6772 - accuracy: 0.7083 - val_loss: 0.7323 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 17: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 17: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4437 - accuracy: 0.8333 - val_loss: 0.7950 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.9167\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 18: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 18: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.3250 - accuracy: 0.9167 - val_loss: 0.7925 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5795 - accuracy: 0.7500\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 19: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 19: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.5795 - accuracy: 0.7500 - val_loss: 0.7617 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4011 - accuracy: 0.7917\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 20: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 20: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 20: saving model to model_2/KERAS_check_model_epoch20.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.4011 - accuracy: 0.7917 - val_loss: 0.7538 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3906 - accuracy: 0.7500\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 21: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 21: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3906 - accuracy: 0.7500 - val_loss: 0.7677 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 22: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 22: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.4255 - accuracy: 0.8333 - val_loss: 0.7785 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 23: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 23: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.3764 - accuracy: 0.8333 - val_loss: 0.7769 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.7917\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 24: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 24: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.4387 - accuracy: 0.7917 - val_loss: 0.7535 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9167\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 25: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 25: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2600 - accuracy: 0.9167 - val_loss: 0.7324 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 26: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 26: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.3831 - accuracy: 0.8333 - val_loss: 0.7238 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 27: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 27: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.3094 - accuracy: 0.8333 - val_loss: 0.7307 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3860 - accuracy: 0.8750\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 28: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 28: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.3860 - accuracy: 0.8750 - val_loss: 0.7426 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.9583\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 29: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 29: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2616 - accuracy: 0.9583 - val_loss: 0.7465 - val_accuracy: 0.6250 - lr: 2.5000e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9583\n",
      "***callbacks***\n",
      "saving losses to model_2/losses.log\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.68320\n",
      "\n",
      "Epoch 30: saving model to model_2/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 30: saving model to model_2/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 30: saving model to model_2/KERAS_check_model_epoch30.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.2884 - accuracy: 0.9583 - val_loss: 0.7458 - val_accuracy: 0.6250 - lr: 2.5000e-04\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(\n",
    "        stop_patience=1000,\n",
    "        lr_factor=0.5,\n",
    "        lr_patience=10,\n",
    "        lr_epsilon=0.000001,\n",
    "        lr_cooldown=2,\n",
    "        lr_minimum=0.0000001,\n",
    "        outputDir='model_2',\n",
    "    )\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=1024,\n",
    "        epochs=30,\n",
    "        validation_split=0.25,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks.callbacks,\n",
    "    )\n",
    "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "    model = strip_pruning(model)\n",
    "    model.save('model_2/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "    model = load_model('model_2/KERAS_check_best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifica la dispersión\n",
    "Realiza una rápida verificación para asegurarte de que el modelo fue entrenado con dispersión. Simplemente haremos un histograma de los pesos de la primera capa y esperamos observar un pico grande en el bin que contiene '0'. Ten en cuenta que el eje y será logarítmico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of zeros = 0.1015625\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = model.layers[1].weights[1].numpy()\n",
    "h, b = np.histogram(w, bins=100)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(b[:-1], h, width=b[1] - b[0])\n",
    "plt.semilogy()\n",
    "print('% of zeros = {}'.format(np.sum(w == 0) / np.size(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifica el rendimiento\n",
    "¿Cómo se compara este modelo disperso al 75% con el modelo no podado? Informemos la precisión y realicemos una curva ROC. El modelo podado se muestra con líneas sólidas, mientras que el modelo no podado de la parte 1 se muestra con líneas discontinuas. Asegúrate de haber entrenado el modelo de la parte 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Accuracy unpruned: 0.875\n",
      "Accuracy pruned:   1.0\n"
     ]
    }
   ],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
    "\n",
    "y_ref = model_ref.predict(X_test)\n",
    "y_prune = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy unpruned: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Accuracy pruned:   {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_prune, axis=1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convierte el modelo en firmware para FPGA con hls4ml.\n",
    "Utilizaremos la configuración predeterminada: precisión ap_fixed<16,6> en todas partes y ReuseFactor=1, para que podamos compararlo con el modelo de la parte 1. Necesitamos usar strip_pruning para cambiar los tipos de capa de vuelta a sus originales.\n",
    "\n",
    "La síntesis llevará un tiempo.\n",
    "\n",
    "Mientras se ejecuta la síntesis C, podemos monitorear el progreso mirando el archivo de registro abriendo una terminal desde la página principal del cuaderno y ejecutando:\n",
    "\n",
    "`tail -f model_2/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_2, layer type: InputLayer, input shapes: [[None, 60, 60]], output shape: [None, 60, 60]\n",
      "Layer name: flatten_1, layer type: Reshape, input shapes: [[None, 60, 60]], output shape: [None, 3600]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 3600]], output shape: [None, 128]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 128]], output shape: [None, 2]\n",
      "{'Model': {'Precision': 'fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}}\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_2, layer type: InputLayer, input shapes: [[None, 60, 60]], output shape: [None, 60, 60]\n",
      "Layer name: flatten_1, layer type: Reshape, input shapes: [[None, 60, 60]], output shape: [None, 3600]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 3600]], output shape: [None, 128]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 128]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /opt/Xilinx/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: Applying HLS Y2K22 patch v1.2 for IP revision\n",
      "INFO: [HLS 200-10] Running '/opt/Xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'jovyan' on host '0839a13bbf90' (Linux_x86_64 version 6.5.0-35-generic) on Mon May 27 02:18:29 UTC 2024\n",
      "INFO: [HLS 200-10] In directory '/home/jovyan/HLS4ML/model_2/hls4ml_prj'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-10] Creating and opening project '/home/jovyan/HLS4ML/model_2/hls4ml_prj/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Creating and opening solution '/home/jovyan/HLS4ML/model_2/hls4ml_prj/myproject_prj/solution1'.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "INFO: [HLS 200-10] Setting target device to 'xcu250-figd2104-2L-e'\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:06:42 ; elapsed = 00:06:55 . Memory (MB): peak = 925.133 ; gain = 521.035 ; free physical = 1456 ; free virtual = 2677\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:06:42 ; elapsed = 00:06:55 . Memory (MB): peak = 925.133 ; gain = 521.035 ; free physical = 1460 ; free virtual = 2683\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "print(config)\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_2/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")\n",
    "hls_model.compile()\n",
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifica reportes\n",
    "Verifica los informes generados por Vivado HLS. Presta atención especialmente a la sección 'Utilization Estimates'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_2/hls4ml_prj/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el informe del modelo entrenado en la parte 1. Recuerda que estos modelos tienen la misma arquitectura, pero el modelo en esta sección fue entrenado usando la API de sparse de tensorflow_model_optimization. Observa cómo el uso de recursos se ha reducido drásticamente (especialmente los DSP). Cuando Vivado HLS detecta una operación como y = 0 * x, puede evitar colocar un DSP para esa operación. El impacto de esto es mayor cuando ReuseFactor = 1, pero aún se aplica incluso en un reuso más alto. Ten en cuenta que debes haber entrenado y sintetizado el modelo de la parte 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Cuantificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el conjunto de datos de etiquetado de jets de Open ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir un modelo\n",
    "Esta vez vamos a utilizar capas de QKeras.\n",
    "QKeras es \"Quantized Keras\" para la cuantificación heterogénea profunda de modelos de ML.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "Está mantenido por Google y recientemente agregamos soporte para modelos de QKeras en hls4ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos utilizando la capa `QDense` en lugar de `Dense`, y `QActivation` en lugar de `Activation`. También estamos especificando `kernel_quantizer = quantized_bits(6,0,0)`. Esto utilizará 6 bits (de los cuales 0 son enteros) para los pesos. También usamos la misma cuantificación para los sesgos, y `quantized_relu(6)` para activaciones ReLU de 6 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    QDense(\n",
    "        64,\n",
    "        input_shape=(16,),\n",
    "        name='fc1',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        32,\n",
    "        name='fc2',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        32,\n",
    "        name='fc3',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        5,\n",
    "        name='output',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar con esparcidad\n",
    "Volvamos a entrenar con esparcidad en el modelo, ya que las capas de QKeras son podables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo\n",
    "Utilizaremos la misma configuración que el modelo de la parte 1: optimizador Adam con pérdida categórica de entropía cruzada.\n",
    "Los callbacks disminuirán la tasa de aprendizaje y guardarán el modelo en un directorio llamado 'model_2'.\n",
    "El modelo no es muy complejo, por lo que esto debería tomar solo unos minutos incluso en la CPU.\n",
    "Si has reiniciado el kernel del cuaderno después de entrenar una vez, establece `train = False` para cargar el modelo entrenado en lugar de entrenar nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(\n",
    "        stop_patience=1000,\n",
    "        lr_factor=0.5,\n",
    "        lr_patience=10,\n",
    "        lr_epsilon=0.000001,\n",
    "        lr_cooldown=2,\n",
    "        lr_minimum=0.0000001,\n",
    "        outputDir='model_3',\n",
    "    )\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=1024,\n",
    "        epochs=30,\n",
    "        validation_split=0.25,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks.callbacks,\n",
    "    )\n",
    "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "    model = strip_pruning(model)\n",
    "    model.save('model_3/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model('model_3/KERAS_check_best_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobar rendimiento\n",
    "¿Cómo se compara este modelo, que fue entrenado usando 6 bits y 75% de esparcidad, con el modelo original? Informemos sobre la precisión y hagamos una curva ROC. El modelo cuantificado y podado se muestra con líneas sólidas, mientras que el modelo no podado de la parte 1 se muestra con líneas discontinuas.\n",
    "\n",
    "También debemos verificar que hls4ml pueda respetar la elección de usar 6 bits en todo el modelo y que coincida con la precisión. Generaremos una configuración a partir de este modelo cuantificado, y graficaremos su rendimiento como la línea punteada.\n",
    "La configuración generada se imprime. Notarás que utiliza 7 bits para el tipo, ¿pero especificamos 6? Eso se debe a que QKeras no cuenta el bit de signo cuando especificamos el número de bits, por lo que el tipo que realmente se utiliza necesita 1 más.\n",
    "\n",
    "También utilizaremos el pase de optimización `OutputRoundingSaturationMode` de `hls4ml` para establecer las capas de activación en redondear, en lugar de truncar, el fundido. Esto es importante para obtener una buena precisión del modelo cuando se utilizan activaciones de precisión de bits pequeños. Y estableceremos un tipo de datos diferente para las tablas utilizadas en el Softmax, solo para un poco de rendimiento adicional.\n",
    "\n",
    "**Asegúrate de haber entrenado el modelo de la parte 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_3/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")\n",
    "hls_model.compile()\n",
    "\n",
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))\n",
    "np.save('model_3/y_qkeras.npy', y_qkeras)\n",
    "np.save('model_3/y_hls.npy', y_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
    "y_ref = model_ref.predict(X_test)\n",
    "\n",
    "print(\"Accuracy baseline:  {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Accuracy pruned, quantized: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
    "print(\"Accuracy hls4ml: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.makeRoc(y_test, y_ref, classes)\n",
    "plt.gca().set_prop_cycle(None)  # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_qkeras, classes, linestyle='--')\n",
    "plt.gca().set_prop_cycle(None)  # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_hls, classes, linestyle=':')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "lines = [Line2D([0], [0], ls='-'), Line2D([0], [0], ls='--'), Line2D([0], [0], ls=':')]\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "leg = Legend(ax, lines, labels=['baseline', 'pruned, quantized', 'hls4ml'], loc='lower right', frameon=False)\n",
    "ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintetizar\n",
    "Ahora vamos a sintetizar este modelo cuantificado y podado.\n",
    "\n",
    "**La síntesis llevará un tiempo**\n",
    "\n",
    "Mientras se ejecuta la síntesis en C, podemos monitorear el progreso observando el archivo de registro abriendo un terminal desde el inicio del cuaderno y ejecutando:\n",
    "\n",
    "`tail -f model_3/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar los informes\n",
    "Imprime los informes generados por Vivado HLS. Presta especial atención a la sección \"Estimaciones de utilización\" esta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_3/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el informe del modelo entrenado en la parte 1. Ahora, en comparación con el modelo de la parte 1, este modelo ha sido entrenado con cuantificación de baja precisión y un 75% de podado. Deberías poder ver que hemos ahorrado muchos recursos en comparación con donde comenzamos en la parte 1. Al mismo tiempo, consultando la curva ROC anterior, ¡el rendimiento del modelo es prácticamente idéntico incluso con esta compresión drástica!\n",
    "\n",
    "**Nota que necesitas haber entrenado y sintetizado el modelo de la parte 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el informe del modelo entrenado en la parte 3. Ambos modelos fueron entrenados con un 75% de esparcidad, pero el nuevo modelo también utiliza una precisión de 6 bits. Puedes ver cómo Vivado HLS ha trasladado las operaciones de multiplicación de DSP a LUTs, reduciendo el uso de recursos \"críticos\".\n",
    "\n",
    "**Nota que necesitas haber entrenado y sintetizado el modelo de la parte 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_2/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota\n",
    "Ten en cuenta también que las estimaciones de recursos de Vivado HLS tienden a _sobreestimar_ las LUTs, mientras que generalmente estiman correctamente los DSPs. Ejecutar las etapas posteriores de compilación de FPGA revela el uso de recursos más realista. Puedes ejecutar el siguiente paso, 'síntesis lógica', con `hls_model.build(synth=True, vsynth=True)`, pero lo hemos omitido en este tutorial por razones de tiempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

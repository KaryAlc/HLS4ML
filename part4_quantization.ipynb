{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Cuantificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 02:20:30.400882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 02:20:30.535136: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el conjunto de datos de etiquetado de jets de Open ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir un modelo\n",
    "Esta vez vamos a utilizar capas de QKeras.\n",
    "QKeras es \"Quantized Keras\" para la cuantificación heterogénea profunda de modelos de ML.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "Está mantenido por Google y recientemente agregamos soporte para modelos de QKeras en hls4ml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos utilizando la capa `QDense` en lugar de `Dense`, y `QActivation` en lugar de `Activation`. También estamos especificando `kernel_quantizer = quantized_bits(6,0,0)`. Esto utilizará 6 bits (de los cuales 0 son enteros) para los pesos. También usamos la misma cuantificación para los sesgos, y `quantized_relu(6)` para activaciones ReLU de 6 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar con esparcidad\n",
    "Volvamos a entrenar con esparcidad en el modelo, ya que las capas de QKeras son podables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo\n",
    "Utilizaremos la misma configuración que el modelo de la parte 1: optimizador Adam con pérdida categórica de entropía cruzada.\n",
    "Los callbacks disminuirán la tasa de aprendizaje y guardarán el modelo en un directorio llamado 'model_2'.\n",
    "El modelo no es muy complejo, por lo que esto debería tomar solo unos minutos incluso en la CPU.\n",
    "Si has reiniciado el kernel del cuaderno después de entrenar una vez, establece `train = False` para cargar el modelo entrenado en lugar de entrenar nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.9003 - accuracy: 0.5833 - val_loss: 0.9154 - val_accuracy: 0.6250\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8847 - accuracy: 0.6667 - val_loss: 0.9128 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8730 - accuracy: 0.7083 - val_loss: 0.9125 - val_accuracy: 0.5000\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8621 - accuracy: 0.7917 - val_loss: 0.9114 - val_accuracy: 0.5000\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8552 - accuracy: 0.8750 - val_loss: 0.9186 - val_accuracy: 0.3750\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8432 - accuracy: 0.9167 - val_loss: 0.9148 - val_accuracy: 0.3750\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8358 - accuracy: 0.9583 - val_loss: 0.9217 - val_accuracy: 0.3750\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.8259 - accuracy: 0.9167 - val_loss: 0.9150 - val_accuracy: 0.2500\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8180 - accuracy: 0.9583 - val_loss: 0.9121 - val_accuracy: 0.3750\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8100 - accuracy: 0.9583 - val_loss: 0.9130 - val_accuracy: 0.3750\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7990 - accuracy: 0.9583 - val_loss: 0.9135 - val_accuracy: 0.5000\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7916 - accuracy: 0.9583 - val_loss: 0.9154 - val_accuracy: 0.5000\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7822 - accuracy: 0.8750 - val_loss: 0.9155 - val_accuracy: 0.5000\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7763 - accuracy: 0.8750 - val_loss: 0.9196 - val_accuracy: 0.5000\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7702 - accuracy: 0.8750 - val_loss: 0.9111 - val_accuracy: 0.3750\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.7623 - accuracy: 0.9167 - val_loss: 0.9062 - val_accuracy: 0.3750\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7533 - accuracy: 0.9583 - val_loss: 0.9033 - val_accuracy: 0.3750\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7471 - accuracy: 0.9167 - val_loss: 0.9021 - val_accuracy: 0.3750\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.7386 - accuracy: 0.9167 - val_loss: 0.9033 - val_accuracy: 0.5000\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.7311 - accuracy: 0.9583 - val_loss: 0.9001 - val_accuracy: 0.6250\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.7226 - accuracy: 0.9583 - val_loss: 0.8976 - val_accuracy: 0.6250\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7129 - accuracy: 0.9583 - val_loss: 0.9042 - val_accuracy: 0.6250\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.7055 - accuracy: 0.9583 - val_loss: 0.8976 - val_accuracy: 0.6250\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6955 - accuracy: 0.9583 - val_loss: 0.8921 - val_accuracy: 0.6250\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6854 - accuracy: 0.9583 - val_loss: 0.8910 - val_accuracy: 0.6250\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6772 - accuracy: 0.9583 - val_loss: 0.8928 - val_accuracy: 0.6250\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6707 - accuracy: 0.9583 - val_loss: 0.8924 - val_accuracy: 0.6250\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6611 - accuracy: 0.9583 - val_loss: 0.8907 - val_accuracy: 0.6250\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6533 - accuracy: 0.9583 - val_loss: 0.8820 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6428 - accuracy: 0.9583 - val_loss: 0.8832 - val_accuracy: 0.6250\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, Flatten\n",
    "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude, UpdatePruningStep, PruningSummaries, strip_pruning, ConstantSparsity\n",
    "from qkeras import QDense, QActivation, quantized_bits, quantized_relu\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "# Definir el modelo con capas cuantizadas y poda\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        64,\n",
    "        input_shape=(60, 60),  # Definir la entrada con forma (60, 60)\n",
    "        name='fc1',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(Flatten())  # Aplana la entrada 2D a 1D\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        32,\n",
    "        name='fc2',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        32,\n",
    "        name='fc3',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        2,\n",
    "        name='output',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "# Entrenamiento\n",
    "train = True\n",
    "if train:\n",
    "    adam = Adam(learning_rate=0.0001)  # Configura el optimizador Adam\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])  # Compila el modelo con pérdida y métricas\n",
    "    \n",
    "    # Define los callbacks para el entrenamiento\n",
    "    callbacks = [\n",
    "        UpdatePruningStep(),  # Actualiza el paso de poda en cada iteración\n",
    "        PruningSummaries(log_dir='logs/pruning')  # Resumen de la poda\n",
    "    ]\n",
    "    \n",
    "    # Entrenamiento del modelo\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=1024,\n",
    "        epochs=30,\n",
    "        validation_split=0.25,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Elimina las operaciones de poda para crear un modelo más eficiente\n",
    "    model = strip_pruning(model)\n",
    "    model.save('model_1/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    \n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model('model_1/KERAS_check_best_model.h5', custom_objects=co)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobar rendimiento\n",
    "¿Cómo se compara este modelo, que fue entrenado usando 6 bits y 75% de esparcidad, con el modelo original? Informemos sobre la precisión y hagamos una curva ROC. El modelo cuantificado y podado se muestra con líneas sólidas, mientras que el modelo no podado de la parte 1 se muestra con líneas discontinuas.\n",
    "\n",
    "También debemos verificar que hls4ml pueda respetar la elección de usar 6 bits en todo el modelo y que coincida con la precisión. Generaremos una configuración a partir de este modelo cuantificado, y graficaremos su rendimiento como la línea punteada.\n",
    "La configuración generada se imprime. Notarás que utiliza 7 bits para el tipo, ¿pero especificamos 6? Eso se debe a que QKeras no cuenta el bit de signo cuando especificamos el número de bits, por lo que el tipo que realmente se utiliza necesita 1 más.\n",
    "\n",
    "También utilizaremos el pase de optimización `OutputRoundingSaturationMode` de `hls4ml` para establecer las capas de activación en redondear, en lugar de truncar, el fundido. Esto es importante para obtener una buena precisión del modelo cuando se utilizan activaciones de precisión de bits pequeños. Y estableceremos un tipo de datos diferente para las tablas utilizadas en el Softmax, solo para un poco de rendimiento adicional.\n",
    "\n",
    "**Asegúrate de haber entrenado el modelo de la parte 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: prune_low_magnitude_fc1_input, layer type: InputLayer, input shapes: [[None, 60, 60]], output shape: [None, 60, 60]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 60, 60]], output shape: [None, 60, 64]\n",
      "Layer name: flatten_1, layer type: Reshape, input shapes: [[None, 60, 64]], output shape: [None, 3840]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 3840]], output shape: [None, 3840]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 3840]], output shape: [None, 32]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 2]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "-----------------------------------\n",
      "Model\n",
      "  Precision:         fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  prune_low_magnitude_fc1_input\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  fc1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<6,1>\n",
      "      bias:          fixed<6,1>\n",
      "  fc1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  flatten_1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  relu1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "  fc2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<6,1>\n",
      "      bias:          fixed<6,1>\n",
      "  fc2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  relu2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "  fc3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<6,1>\n",
      "      bias:          fixed<6,1>\n",
      "  fc3_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  relu3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "  output\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<6,1>\n",
      "      bias:          fixed<6,1>\n",
      "  output_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  softmax\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    exp_table_t:     ap_fixed<18,8>\n",
      "    inv_table_t:     ap_fixed<18,4>\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: prune_low_magnitude_fc1_input, layer type: InputLayer, input shapes: [[None, 60, 60]], output shape: [None, 60, 60]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 60, 60]], output shape: [None, 60, 64]\n",
      "Layer name: flatten_1, layer type: Reshape, input shapes: [[None, 60, 64]], output shape: [None, 3840]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 3840]], output shape: [None, 3840]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 3840]], output shape: [None, 32]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 2]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Layer fc1 requires \"dataflow\" pipeline style. Switching to \"dataflow\" pipeline style.\n",
      "Writing HLS project\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "1/1 [==============================] - 0s 426ms/step\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_3/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")\n",
    "hls_model.compile()\n",
    "\n",
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))\n",
    "np.save('model_3/y_qkeras.npy', y_qkeras)\n",
    "np.save('model_3/y_hls.npy', y_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 618ms/step\n",
      "Accuracy baseline:  1.0\n",
      "Accuracy pruned, quantized: 1.0\n",
      "Accuracy hls4ml: 0.875\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from qkeras import QDense, QActivation, quantized_relu  # Importar todas las capas y activaciones personalizadas\n",
    "\n",
    "# Usar el contexto custom_object_scope para registrar todas las capas y activaciones personalizadas\n",
    "with tf.keras.utils.custom_object_scope({'QDense': QDense, 'QActivation': QActivation, 'quantized_relu': quantized_relu}):\n",
    "    model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
    "\n",
    "y_ref = model_ref.predict(X_test)\n",
    "\n",
    "print(\"Accuracy baseline:  {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Accuracy pruned, quantized: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
    "print(\"Accuracy hls4ml: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintetizar\n",
    "Ahora vamos a sintetizar este modelo cuantificado y podado.\n",
    "\n",
    "**La síntesis llevará un tiempo**\n",
    "\n",
    "Mientras se ejecuta la síntesis en C, podemos monitorear el progreso observando el archivo de registro abriendo un terminal desde el inicio del cuaderno y ejecutando:\n",
    "\n",
    "`tail -f model_3/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /opt/Xilinx/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: Applying HLS Y2K22 patch v1.2 for IP revision\n",
      "INFO: [HLS 200-10] Running '/opt/Xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'jovyan' on host 'e0373710e214' (Linux_x86_64 version 6.5.0-35-generic) on Tue May 28 02:29:46 UTC 2024\n",
      "INFO: [HLS 200-10] In directory '/home/jovyan/HLS4ML/model_3/hls4ml_prj'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-10] Creating and opening project '/home/jovyan/HLS4ML/model_3/hls4ml_prj/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Creating and opening solution '/home/jovyan/HLS4ML/model_3/hls4ml_prj/myproject_prj/solution1'.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "INFO: [HLS 200-10] Setting target device to 'xcu250-figd2104-2L-e'\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:41:103\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:41:108\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:50:67\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:50:71\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:58:67\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:58:71\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:66:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:66:77\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 8 issue(s) in file firmware/myproject.cpp\n",
      "INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:02:13 ; elapsed = 00:02:22 . Memory (MB): peak = 861.133 ; gain = 457.035 ; free physical = 1962 ; free virtual = 3188\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:02:13 ; elapsed = 00:02:22 . Memory (MB): peak = 861.133 ; gain = 457.035 ; free physical = 1964 ; free virtual = 3191\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv1d_latency.h:40) in function 'void nnet::conv_1d_latency_cl<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::conv_1d_latency_cl<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' (firmware/nnet_utils/nnet_conv1d_latency.h:54).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_1d_latency_cl<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' into 'nnet::pointwise_conv_1d_cl<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' (firmware/nnet_utils/nnet_conv1d.h:58).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' into 'myproject' (firmware/myproject.cpp:50).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' into 'myproject' (firmware/myproject.cpp:58).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' into 'myproject' (firmware/myproject.cpp:66).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::softmax<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' into 'myproject' (firmware/myproject.cpp:68).\n",
      "INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:13:20 ; elapsed = 00:14:15 . Memory (MB): peak = 1373.133 ; gain = 969.035 ; free physical = 1158 ; free virtual = 2169\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config15_mult>' into 'nnet::pointwise_conv_1d_cl<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' (firmware/nnet_utils/nnet_conv1d_latency.h:81->firmware/nnet_utils/nnet_conv1d.h:58) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' into 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' (firmware/nnet_utils/nnet_dense_latency.h:66) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' into 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_latency.h:66) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' into 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' (firmware/nnet_utils/nnet_dense_latency.h:66) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' (firmware/nnet_utils/nnet_activation.h:239) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' (firmware/nnet_utils/nnet_activation.h:254) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' (firmware/nnet_utils/nnet_activation.h:262) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' (firmware/nnet_utils/nnet_activation.h:265) automatically.\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:14:07 ; elapsed = 00:15:03 . Memory (MB): peak = 1373.133 ; gain = 969.035 ; free physical = 1871 ; free virtual = 3513\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' (firmware/nnet_utils/nnet_activation.h:217:46).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' (firmware/nnet_utils/nnet_dense_latency.h:17:48).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, relu_config11>' (firmware/nnet_utils/nnet_activation.h:40:13).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_latency.h:17:48).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, relu_config8>' (firmware/nnet_utils/nnet_activation.h:40:13).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' (firmware/nnet_utils/nnet_dense_latency.h:17:48).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, relu_config5>' (firmware/nnet_utils/nnet_activation.h:40:13).\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv1d_latency.h:34) in function 'nnet::pointwise_conv_1d_cl<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config15>' for pipelining.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:243) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-2' (firmware/nnet_utils/nnet_activation.h:252) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-3' (firmware/nnet_utils/nnet_activation.h:266) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config14>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, relu_config11>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, relu_config8>' completely with a factor of 32.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' completely with a factor of 3840.\n",
      "ERROR: [XFORM 203-504] Stop unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::dense_latency<ap_ufixed<6, 0, (ap_q_mode)4, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' because it may cause large runtime and excessive memory usage due to increase in code size. Please avoid unrolling the loop or form sub-functions for code in the loop body.\n",
      "ERROR: [HLS 200-70] Pre-synthesis failed.\n",
      "command 'ap_source' returned error code\n",
      "    while executing\n",
      "\"source build_prj.tcl\"\n",
      "    (\"uplevel\" body line 1)\n",
      "    invoked from within\n",
      "\"uplevel \\#0 [list source $arg] \"\n",
      "\n",
      "INFO: [Common 17-206] Exiting vivado_hls at Tue May 28 02:45:41 2024...\n",
      "CSynthesis report not found.\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar los informes\n",
    "Imprime los informes generados por Vivado HLS. Presta especial atención a la sección \"Estimaciones de utilización\" esta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in model_3/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('model_3/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el informe del modelo entrenado en la parte 1. Ahora, en comparación con el modelo de la parte 1, este modelo ha sido entrenado con cuantificación de baja precisión y un 75% de podado. Deberías poder ver que hemos ahorrado muchos recursos en comparación con donde comenzamos en la parte 1. Al mismo tiempo, consultando la curva ROC anterior, ¡el rendimiento del modelo es prácticamente idéntico incluso con esta compresión drástica!\n",
    "\n",
    "**Nota que necesitas haber entrenado y sintetizado el modelo de la parte 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in model_1/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el informe del modelo entrenado en la parte 3. Ambos modelos fueron entrenados con un 75% de esparcidad, pero el nuevo modelo también utiliza una precisión de 6 bits. Puedes ver cómo Vivado HLS ha trasladado las operaciones de multiplicación de DSP a LUTs, reduciendo el uso de recursos \"críticos\".\n",
    "\n",
    "**Nota que necesitas haber entrenado y sintetizado el modelo de la parte 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in model_2/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('model_2/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota\n",
    "Ten en cuenta también que las estimaciones de recursos de Vivado HLS tienden a _sobreestimar_ las LUTs, mientras que generalmente estiman correctamente los DSPs. Ejecutar las etapas posteriores de compilación de FPGA revela el uso de recursos más realista. Puedes ejecutar el siguiente paso, 'síntesis lógica', con `hls_model.build(synth=True, vsynth=True)`, pero lo hemos omitido en este tutorial por razones de tiempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Cuantificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el conjunto de datos de etiquetado de jets de Open ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir un modelo\n",
    "Esta vez vamos a utilizar capas de QKeras.\n",
    "QKeras es \"Quantized Keras\" para la cuantificación heterogénea profunda de modelos de ML.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "Está mantenido por Google y recientemente agregamos soporte para modelos de QKeras en hls4ml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos utilizando la capa `QDense` en lugar de `Dense`, y `QActivation` en lugar de `Activation`. También estamos especificando `kernel_quantizer = quantized_bits(6,0,0)`. Esto utilizará 6 bits (de los cuales 0 son enteros) para los pesos. También usamos la misma cuantificación para los sesgos, y `quantized_relu(6)` para activaciones ReLU de 6 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar con esparcidad\n",
    "Volvamos a entrenar con esparcidad en el modelo, ya que las capas de QKeras son podables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo\n",
    "Utilizaremos la misma configuración que el modelo de la parte 1: optimizador Adam con pérdida categórica de entropía cruzada.\n",
    "Los callbacks disminuirán la tasa de aprendizaje y guardarán el modelo en un directorio llamado 'model_2'.\n",
    "El modelo no es muy complejo, por lo que esto debería tomar solo unos minutos incluso en la CPU.\n",
    "Si has reiniciado el kernel del cuaderno después de entrenar una vez, establece `train = False` para cargar el modelo entrenado en lugar de entrenar nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.9487 - accuracy: 0.4583 - val_loss: 0.9278 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.9053 - accuracy: 0.5417 - val_loss: 0.9154 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.8761 - accuracy: 0.5417 - val_loss: 0.9012 - val_accuracy: 0.6250\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.8388 - accuracy: 0.6667 - val_loss: 0.8982 - val_accuracy: 0.7500\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.8126 - accuracy: 0.8333 - val_loss: 0.8956 - val_accuracy: 0.7500\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.7663 - accuracy: 0.8333 - val_loss: 0.8621 - val_accuracy: 0.7500\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.7383 - accuracy: 0.8333 - val_loss: 0.8440 - val_accuracy: 0.7500\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.7090 - accuracy: 0.8750 - val_loss: 0.8308 - val_accuracy: 0.7500\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6845 - accuracy: 0.9167 - val_loss: 0.8145 - val_accuracy: 0.7500\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.6599 - accuracy: 0.9583 - val_loss: 0.8008 - val_accuracy: 0.7500\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6315 - accuracy: 0.9583 - val_loss: 0.8213 - val_accuracy: 0.6250\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.6035 - accuracy: 0.9583 - val_loss: 0.8207 - val_accuracy: 0.6250\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.5770 - accuracy: 1.0000 - val_loss: 0.8230 - val_accuracy: 0.6250\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.5619 - accuracy: 1.0000 - val_loss: 0.8194 - val_accuracy: 0.6250\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.5463 - accuracy: 1.0000 - val_loss: 0.8253 - val_accuracy: 0.6250\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5283 - accuracy: 1.0000 - val_loss: 0.8282 - val_accuracy: 0.6250\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5106 - accuracy: 1.0000 - val_loss: 0.8244 - val_accuracy: 0.6250\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4928 - accuracy: 1.0000 - val_loss: 0.8065 - val_accuracy: 0.6250\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4778 - accuracy: 1.0000 - val_loss: 0.8053 - val_accuracy: 0.6250\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4570 - accuracy: 1.0000 - val_loss: 0.8071 - val_accuracy: 0.6250\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4445 - accuracy: 1.0000 - val_loss: 0.8023 - val_accuracy: 0.6250\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4338 - accuracy: 1.0000 - val_loss: 0.7981 - val_accuracy: 0.6250\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4231 - accuracy: 1.0000 - val_loss: 0.7951 - val_accuracy: 0.6250\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4094 - accuracy: 1.0000 - val_loss: 0.8046 - val_accuracy: 0.6250\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3980 - accuracy: 1.0000 - val_loss: 0.8170 - val_accuracy: 0.6250\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3792 - accuracy: 1.0000 - val_loss: 0.8118 - val_accuracy: 0.6250\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3735 - accuracy: 1.0000 - val_loss: 0.8026 - val_accuracy: 0.6250\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.3641 - accuracy: 1.0000 - val_loss: 0.7995 - val_accuracy: 0.6250\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3531 - accuracy: 1.0000 - val_loss: 0.8030 - val_accuracy: 0.6250\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.3447 - accuracy: 1.0000 - val_loss: 0.8120 - val_accuracy: 0.6250\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, Flatten\n",
    "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude, UpdatePruningStep, PruningSummaries, strip_pruning, ConstantSparsity\n",
    "from qkeras import QDense, QActivation, quantized_bits, quantized_relu\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "# Definir el modelo con capas cuantizadas y poda\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        64,\n",
    "        input_shape=(60, 60),  # Definir la entrada con forma (60, 60)\n",
    "        name='fc1',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(Flatten())  # Aplana la entrada 2D a 1D\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        32,\n",
    "        name='fc2',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        32,\n",
    "        name='fc3',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
    "model.add(\n",
    "    prune_low_magnitude(QDense(\n",
    "        2,\n",
    "        name='output',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    ), pruning_schedule=ConstantSparsity(0.5, begin_step=0))\n",
    ")\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "# Entrenamiento\n",
    "train = True\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)  # Configura el optimizador Adam\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])  # Compila el modelo con pérdida y métricas\n",
    "    \n",
    "    # Define los callbacks para el entrenamiento\n",
    "    callbacks = [\n",
    "        UpdatePruningStep(),  # Actualiza el paso de poda en cada iteración\n",
    "        PruningSummaries(log_dir='logs/pruning')  # Resumen de la poda\n",
    "    ]\n",
    "    \n",
    "    # Entrenamiento del modelo\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=1024,\n",
    "        epochs=30,\n",
    "        validation_split=0.25,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Elimina las operaciones de poda para crear un modelo más eficiente\n",
    "    model = strip_pruning(model)\n",
    "    model.save('model_1/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    \n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model('model_1/KERAS_check_best_model.h5', custom_objects=co)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobar rendimiento\n",
    "¿Cómo se compara este modelo, que fue entrenado usando 6 bits y 75% de esparcidad, con el modelo original? Informemos sobre la precisión y hagamos una curva ROC. El modelo cuantificado y podado se muestra con líneas sólidas, mientras que el modelo no podado de la parte 1 se muestra con líneas discontinuas.\n",
    "\n",
    "También debemos verificar que hls4ml pueda respetar la elección de usar 6 bits en todo el modelo y que coincida con la precisión. Generaremos una configuración a partir de este modelo cuantificado, y graficaremos su rendimiento como la línea punteada.\n",
    "La configuración generada se imprime. Notarás que utiliza 7 bits para el tipo, ¿pero especificamos 6? Eso se debe a que QKeras no cuenta el bit de signo cuando especificamos el número de bits, por lo que el tipo que realmente se utiliza necesita 1 más.\n",
    "\n",
    "También utilizaremos el pase de optimización `OutputRoundingSaturationMode` de `hls4ml` para establecer las capas de activación en redondear, en lugar de truncar, el fundido. Esto es importante para obtener una buena precisión del modelo cuando se utilizan activaciones de precisión de bits pequeños. Y estableceremos un tipo de datos diferente para las tablas utilizadas en el Softmax, solo para un poco de rendimiento adicional.\n",
    "\n",
    "**Asegúrate de haber entrenado el modelo de la parte 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotting\u001b[39;00m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m hls4ml\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mconfig_from_keras_model(model, granularity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hls4ml/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converters, report, utils  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hls4ml/converters/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_to_hls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasFileReader  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_to_hls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasModelReader  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_to_hls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasReader  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hls4ml/converters/keras_to_hls.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelGraph\n\u001b[1;32m      7\u001b[0m MAXMULT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasReader\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hls4ml/model/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HLSConfig, ModelGraph  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhls4ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m profiling  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     __profiling_enabled__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hls4ml/model/profiling.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m     __tf_profiling_enabled__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     __torch_profiling_enabled__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:560\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    562\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_3/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")\n",
    "hls_model.compile()\n",
    "\n",
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))\n",
    "np.save('model_3/y_qkeras.npy', y_qkeras)\n",
    "np.save('model_3/y_hls.npy', y_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
    "y_ref = model_ref.predict(X_test)\n",
    "\n",
    "print(\"Accuracy baseline:  {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Accuracy pruned, quantized: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
    "print(\"Accuracy hls4ml: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.makeRoc(y_test, y_ref, classes)\n",
    "plt.gca().set_prop_cycle(None)  # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_qkeras, classes, linestyle='--')\n",
    "plt.gca().set_prop_cycle(None)  # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_hls, classes, linestyle=':')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "lines = [Line2D([0], [0], ls='-'), Line2D([0], [0], ls='--'), Line2D([0], [0], ls=':')]\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "leg = Legend(ax, lines, labels=['baseline', 'pruned, quantized', 'hls4ml'], loc='lower right', frameon=False)\n",
    "ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintetizar\n",
    "Ahora vamos a sintetizar este modelo cuantificado y podado.\n",
    "\n",
    "**La síntesis llevará un tiempo**\n",
    "\n",
    "Mientras se ejecuta la síntesis en C, podemos monitorear el progreso observando el archivo de registro abriendo un terminal desde el inicio del cuaderno y ejecutando:\n",
    "\n",
    "`tail -f model_3/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar los informes\n",
    "Imprime los informes generados por Vivado HLS. Presta especial atención a la sección \"Estimaciones de utilización\" esta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_3/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el informe del modelo entrenado en la parte 1. Ahora, en comparación con el modelo de la parte 1, este modelo ha sido entrenado con cuantificación de baja precisión y un 75% de podado. Deberías poder ver que hemos ahorrado muchos recursos en comparación con donde comenzamos en la parte 1. Al mismo tiempo, consultando la curva ROC anterior, ¡el rendimiento del modelo es prácticamente idéntico incluso con esta compresión drástica!\n",
    "\n",
    "**Nota que necesitas haber entrenado y sintetizado el modelo de la parte 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el informe del modelo entrenado en la parte 3. Ambos modelos fueron entrenados con un 75% de esparcidad, pero el nuevo modelo también utiliza una precisión de 6 bits. Puedes ver cómo Vivado HLS ha trasladado las operaciones de multiplicación de DSP a LUTs, reduciendo el uso de recursos \"críticos\".\n",
    "\n",
    "**Nota que necesitas haber entrenado y sintetizado el modelo de la parte 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_2/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota\n",
    "Ten en cuenta también que las estimaciones de recursos de Vivado HLS tienden a _sobreestimar_ las LUTs, mientras que generalmente estiman correctamente los DSPs. Ejecutar las etapas posteriores de compilación de FPGA revela el uso de recursos más realista. Puedes ejecutar el siguiente paso, 'síntesis lógica', con `hls_model.build(synth=True, vsynth=True)`, pero lo hemos omitido en este tutorial por razones de tiempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

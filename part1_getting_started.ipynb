{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Comenzando\n",
    "## Importación de bibliotecas y configuración de entorno: \n",
    "Se importan las bibliotecas necesarias, como TensorFlow, sklearn y numpy. Además, se establece la semilla para la reproducibilidad de los resultados y se configura la variable de entorno 'PATH' para el entorno de Vivado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 02:18:03.135398: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 02:18:03.274620: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención y preparación de datos: \n",
    "Este código carga imágenes de entrenamiento y prueba desde carpetas específicas, las redimensiona a un tamaño deseado y las almacena junto con sus etiquetas en matrices numpy para su posterior procesamiento. Los datos se dividen en imagenes (X) y etiquetas (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow y tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Librerias de ayuda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.24.3)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "Dimensiones de las imágenes: (40, 60, 60)\n",
      "Dimensiones de las etiquetas: (40,)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Tamaño deseado para todas las imágenes\n",
    "nuevo_ancho = 60\n",
    "nuevo_alto = 60\n",
    "\n",
    "# Ruta a la carpeta de imágenes\n",
    "data_dir = \"./signals\"\n",
    "\n",
    "# Función para cargar imágenes y etiquetas de una sola carpeta\n",
    "def load_images_and_labels(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder_name in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for image_file in os.listdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                # Leer la imagen en escala de grises\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                # Redimensionar la imagen\n",
    "                image = cv2.resize(image, (nuevo_ancho, nuevo_alto))\n",
    "                # Normalizar la imagen\n",
    "                image = image / 255.0\n",
    "                images.append(image)\n",
    "                # Usar el nombre de la carpeta como etiqueta\n",
    "                labels.append(folder_name)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Cargar imágenes y etiquetas de la carpeta de datos\n",
    "images, labels = load_images_and_labels(data_dir)\n",
    "\n",
    "# Crear un diccionario con los datos\n",
    "data = {'data': images, 'target': labels}\n",
    "\n",
    "# Organizar los datos según la estructura deseada\n",
    "X, y = data['data'], data['target']\n",
    "\n",
    "# Verificar las dimensiones de los datos cargados\n",
    "print(\"Dimensiones de las imágenes:\", X.shape)\n",
    "print(\"Dimensiones de las etiquetas:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimir información sobre el conjunto de datos: \n",
    "Imprime la forma (shape) del conjunto de datos `X` y las etiquetas `y` utilizando `X.shape` y `y.shape`. Además, imprime las primeras cinco filas de características y etiquetas utilizando `X[:5]` y `y[:5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 60, 60) (40,)\n",
      "[[[0.79215686 0.8627451  0.89411765 ... 0.10588235 0.2745098  0.85098039]\n",
      "  [0.6745098  0.81176471 0.89019608 ... 0.11764706 0.26666667 0.83137255]\n",
      "  [0.60784314 0.7372549  0.8745098  ... 0.10980392 0.34901961 0.81960784]\n",
      "  ...\n",
      "  [0.4        0.40784314 0.38039216 ... 0.54117647 0.66666667 0.67058824]\n",
      "  [0.52156863 0.36862745 0.37254902 ... 0.53333333 0.68235294 0.67843137]\n",
      "  [0.54901961 0.65098039 0.55294118 ... 0.52156863 0.68235294 0.69019608]]\n",
      "\n",
      " [[0.22745098 0.19607843 0.14901961 ... 0.8627451  0.39215686 0.8627451 ]\n",
      "  [0.25490196 0.24313725 0.24705882 ... 0.64705882 0.49803922 0.60784314]\n",
      "  [0.32156863 0.25882353 0.20392157 ... 0.33333333 0.45490196 0.76862745]\n",
      "  ...\n",
      "  [0.10196078 0.05098039 0.37647059 ... 0.4745098  0.35686275 0.55686275]\n",
      "  [0.23921569 0.11372549 0.43529412 ... 0.62352941 0.77647059 0.81568627]\n",
      "  [0.18823529 0.15686275 0.49411765 ... 0.6745098  0.83921569 0.88235294]]\n",
      "\n",
      " [[0.31764706 0.30980392 0.28235294 ... 0.34509804 0.29411765 0.49803922]\n",
      "  [0.36470588 0.31764706 0.31764706 ... 0.35294118 0.34509804 0.43921569]\n",
      "  [0.36470588 0.35686275 0.36862745 ... 0.38823529 0.38039216 0.39607843]\n",
      "  ...\n",
      "  [0.2627451  0.23137255 0.24705882 ... 0.26666667 0.43529412 0.46666667]\n",
      "  [0.39607843 0.41176471 0.38823529 ... 0.54509804 0.5372549  0.36470588]\n",
      "  [0.48235294 0.46666667 0.42352941 ... 0.41568627 0.28627451 0.13333333]]\n",
      "\n",
      " [[0.12156863 0.12156863 0.12941176 ... 0.27058824 0.30588235 0.25098039]\n",
      "  [0.10980392 0.11372549 0.11764706 ... 0.24313725 0.25882353 0.28235294]\n",
      "  [0.09803922 0.10196078 0.10980392 ... 0.25882353 0.24705882 0.20392157]\n",
      "  ...\n",
      "  [0.02745098 0.03921569 0.0745098  ... 0.03137255 0.02352941 0.01568627]\n",
      "  [0.04313725 0.02745098 0.07843137 ... 0.02745098 0.02745098 0.03137255]\n",
      "  [0.05490196 0.0745098  0.08235294 ... 0.05882353 0.04705882 0.03137255]]\n",
      "\n",
      " [[0.12941176 0.1254902  0.12156863 ... 0.03921569 0.05490196 0.08235294]\n",
      "  [0.59607843 0.59215686 0.61568627 ... 0.27058824 0.30196078 0.2745098 ]\n",
      "  [0.6        0.59607843 0.61176471 ... 0.37254902 0.31372549 0.23529412]\n",
      "  ...\n",
      "  [0.4        0.40392157 0.38431373 ... 0.30196078 0.24313725 0.1254902 ]\n",
      "  [0.4        0.40392157 0.38823529 ... 0.33333333 0.25098039 0.1254902 ]\n",
      "  [0.05098039 0.05490196 0.08235294 ... 0.10588235 0.05098039 0.05098039]]]\n",
      "['no_estacionarse' 'no_estacionarse' 'no_estacionarse' 'no_estacionarse'\n",
      " 'no_estacionarse' 'no_estacionarse' 'no_estacionarse' 'no_estacionarse'\n",
      " 'no_estacionarse' 'no_estacionarse' 'no_estacionarse' 'no_estacionarse'\n",
      " 'no_estacionarse' 'no_estacionarse' 'no_estacionarse' 'no_estacionarse'\n",
      " 'no_estacionarse' 'no_estacionarse' 'no_estacionarse' 'no_estacionarse'\n",
      " 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso'\n",
      " 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso'\n",
      " 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso'\n",
      " 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso'\n",
      " 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso' 'ceda_el_paso']\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "print(X[:5])\n",
    "print(y[:40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este bloque de código, se realiza la codificación de las etiquetas `y` para el entrenamiento del modelo. Primero, se utiliza `LabelEncoder()` para transformar las etiquetas de clase de cadena a numérico. Luego, se aplica `to_categorical` para convertir estas etiquetas numéricas en un formato binario \"One Hot\" para la clasificación multiclase. Después, se divide el conjunto de datos en conjuntos de entrenamiento y prueba utilizando `train_test_split`, con un tamaño de prueba del 20% y una semilla aleatoria de 42 para la reproducibilidad de los resultados.\n",
    "\n",
    "Además, se realiza el escalado de las características del conjunto de datos utilizando `StandardScaler()` para asegurar que todas las características tengan una media de cero y una desviación estándar de uno, lo que puede mejorar el rendimiento del modelo. \n",
    "\n",
    "### Archivos creados:\n",
    "Se guardan los conjuntos de datos de entrenamiento y prueba, así como las etiquetas, en archivos `.npy`, junto con la clase original de las etiquetas.\n",
    "* x_train_val.npy\n",
    "* x_test.npy\n",
    "* y_train_val.npy\n",
    "* y_test.npy\n",
    "* classes.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Aplanar cada imagen\n",
    "X_train_val_flat = X_train_val.reshape(X_train_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Escalar cada dimensión por separado\n",
    "scaler = StandardScaler()\n",
    "X_train_val_scaled = scaler.fit_transform(X_train_val_flat)\n",
    "X_test_scaled = scaler.transform(X_test_flat)\n",
    "\n",
    "# Remodelar los datos escalados a su forma original\n",
    "X_train_val_scaled = X_train_val_scaled.reshape(X_train_val.shape)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_val.npy', X_train_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train_val.npy', y_train_val)\n",
    "np.save('y_test.npy', y_test)\n",
    "np.save('classes.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de modelo\n",
    "Se construye un modelo de red neuronal con tres capas ocultas, cada una con activación ReLU, seguidas de una capa de salida con activación softmax. Las capas ocultas tienen 64, 32 y 32 neuronas respectivamente, mientras que la capa de salida tiene 5 neuronas para clasificar en cinco clases diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el modelo\n",
    "Utilizaremos el optimizador Adam con una pérdida de entropía cruzada categórica. Los callbacks disminuirán la tasa de aprendizaje y guardarán el modelo en un directorio llamado 'model_1'. El modelo no es muy complejo, así que esto debería tomar solo unos minutos incluso en la CPU. Si has reiniciado el kernel del notebook después de entrenar una vez, establece `train = False` para cargar el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7257 - accuracy: 0.1667\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.83685, saving model to model_1/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.83685, saving model to model_1/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 1: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 1: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 1.7257 - accuracy: 0.1667 - val_loss: 0.8369 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9297 - accuracy: 0.5000\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 2: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 2: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9297 - accuracy: 0.5000 - val_loss: 1.4048 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1978 - accuracy: 0.5417\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 3: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 3: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1978 - accuracy: 0.5417 - val_loss: 1.2865 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2424 - accuracy: 0.6250\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 4: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 4: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2424 - accuracy: 0.6250 - val_loss: 0.9178 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8310 - accuracy: 0.7500\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 5: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 5: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8310 - accuracy: 0.7500 - val_loss: 1.1920 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8854 - accuracy: 0.7083\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 6: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 6: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8854 - accuracy: 0.7083 - val_loss: 1.5093 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8405 - accuracy: 0.6667\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 7: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 7: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8405 - accuracy: 0.6667 - val_loss: 0.9611 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0093 - accuracy: 0.5000\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 8: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 8: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0093 - accuracy: 0.5000 - val_loss: 0.8748 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.6667\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 9: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 9: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6870 - accuracy: 0.6667 - val_loss: 1.0390 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9641 - accuracy: 0.6250\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.83685\n",
      "\n",
      "Epoch 10: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 10: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 10: saving model to model_1/KERAS_check_model_epoch10.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9641 - accuracy: 0.6250 - val_loss: 0.9638 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5236 - accuracy: 0.7917\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 11: val_loss improved from 0.83685 to 0.78128, saving model to model_1/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.83685 to 0.78128, saving model to model_1/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 11: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 11: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5236 - accuracy: 0.7917 - val_loss: 0.7813 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7379 - accuracy: 0.6667\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 12: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 12: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7379 - accuracy: 0.6667 - val_loss: 0.8146 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.9583\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 13: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 13: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1881 - accuracy: 0.9583 - val_loss: 0.9103 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8178 - accuracy: 0.5417\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 14: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 14: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8178 - accuracy: 0.5417 - val_loss: 0.7989 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.5833\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 15: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 15: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8434 - accuracy: 0.5833 - val_loss: 0.8231 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.9167\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 16: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 16: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.4462 - accuracy: 0.9167 - val_loss: 0.8980 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4926 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 17: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 17: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4926 - accuracy: 0.8333 - val_loss: 0.9943 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.7500\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 18: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 18: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4513 - accuracy: 0.7500 - val_loss: 0.9293 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 19: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 19: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4484 - accuracy: 0.8333 - val_loss: 0.8927 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.8750\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 20: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 20: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 20: saving model to model_1/KERAS_check_model_epoch20.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2683 - accuracy: 0.8750 - val_loss: 0.8562 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.8333\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 21: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 21: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4139 - accuracy: 0.8333 - val_loss: 0.8883 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.8750\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 22: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 22: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2436 - accuracy: 0.8750 - val_loss: 0.8984 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.8750\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 23: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 23: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2000 - accuracy: 0.8750 - val_loss: 0.9038 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.8750\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 24: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 24: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2843 - accuracy: 0.8750 - val_loss: 0.9058 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 1.0000\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 25: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 25: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1267 - accuracy: 1.0000 - val_loss: 0.9050 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9167\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 26: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 26: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2183 - accuracy: 0.9167 - val_loss: 0.9113 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9167\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 27: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 27: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1760 - accuracy: 0.9167 - val_loss: 0.9240 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9583\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 28: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 28: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1581 - accuracy: 0.9583 - val_loss: 0.9303 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.8750\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 29: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 29: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.9312 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8750\n",
      "***callbacks***\n",
      "saving losses to model_1/losses.log\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.78128\n",
      "\n",
      "Epoch 30: saving model to model_1/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 30: saving model to model_1/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 30: saving model to model_1/KERAS_check_model_epoch30.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3299 - accuracy: 0.8750 - val_loss: 0.9376 - val_accuracy: 0.6250 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Input\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(60, 60)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(\n",
    "        stop_patience=1000,\n",
    "        lr_factor=0.5,\n",
    "        lr_patience=10,\n",
    "        lr_epsilon=0.000001,\n",
    "        lr_cooldown=2,\n",
    "        lr_minimum=0.0000001,\n",
    "        outputDir='model_1',\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=1024,\n",
    "        epochs=30,\n",
    "        validation_split=0.25,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks.callbacks,\n",
    "    )\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "    model = load_model('model_1/KERAS_check_best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion del rendimiento\n",
    "Esta sección evalúa el rendimiento del modelo mediante la verificación de la precisión y la creación de una curva ROC.\n",
    "Se utiliza la función accuracy_score de scikit-learn para calcular la precisión del modelo. Luego, se genera una curva ROC utilizando la función makeRoc del módulo de visualización, lo que proporciona una representación gráfica del rendimiento del modelo en diferentes umbrales de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/HLS4ML/plotting.py:74: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if 'j_index' in labels:\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U5')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy_score(np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39margmax(y_keras, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))))\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m9\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mplotting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakeRoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_keras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLS4ML/plotting.py:77\u001b[0m, in \u001b[0;36mmakeRoc\u001b[0;34m(y, predict_test, labels, linestyle, legend)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj_index\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m     75\u001b[0m     labels\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj_index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m fpr, tpr, auc1 \u001b[38;5;241m=\u001b[39m \u001b[43mrocData\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m plotRoc(fpr, tpr, auc1, labels, linestyle, legend\u001b[38;5;241m=\u001b[39mlegend)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predict_test\n",
      "File \u001b[0;32m~/HLS4ML/plotting.py:65\u001b[0m, in \u001b[0;36mrocData\u001b[0;34m(y, predict_test, labels)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n\u001b[1;32m     64\u001b[0m     df[label] \u001b[38;5;241m=\u001b[39m y[:, i]\n\u001b[0;32m---> 65\u001b[0m     df[\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_pred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m] \u001b[38;5;241m=\u001b[39m predict_test[:, i]\n\u001b[1;32m     67\u001b[0m     fpr[label], tpr[label], threshold \u001b[38;5;241m=\u001b[39m roc_curve(df[label], df[label \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_pred\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     69\u001b[0m     auc1[label] \u001b[38;5;241m=\u001b[39m auc(fpr[label], tpr[label])\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U5')) -> None"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_keras = model.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
    "plt.figure(figsize=(9, 9))\n",
    "_ = plotting.makeRoc(y_test, y_keras, le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir el modelo en firmware FPGA con hls4ml\n",
    "Ahora pasaremos por los pasos para convertir el modelo que entrenamos en un firmware FPGA optimizado de baja latencia con hls4ml. Primero, evaluaremos su rendimiento de clasificación para asegurarnos de que no hayamos perdido precisión utilizando tipos de datos de punto fijo. Luego, sintetizaremos el modelo con Vivado HLS y verificaremos las métricas de latencia y uso de recursos de la FPGA.\n",
    "\n",
    "### Crear una configuración y un modelo de hls4ml\n",
    "La biblioteca de inferencia de redes neuronales hls4ml se controla a través de un diccionario de configuración. En este ejemplo, utilizaremos la variación más simple; en ejercicios posteriores se verán configuraciones más avanzadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_1/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiza la arquitectura del modelo generado por hls4ml, muestra las formas de las capas y los tipos de datos.La visualización se genera utilizando la función plot_model de hls4ml.utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar, predecir\n",
    "Ahora necesitamos asegurarnos de que el rendimiento de este modelo siga siendo bueno. Compilamos el hls_model, y luego usamos hls_model.predict para ejecutar el firmware de la FPGA con emulación bit-precisa en la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.compile()\n",
    "X_test = np.ascontiguousarray(X_test)\n",
    "y_hls = hls_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación:\n",
    "Después de comparar, calculamos la precisión de ambos modelos utilizando la función `accuracy_score` de scikit-learn. Luego, creamos un nuevo gráfico para comparar las curvas ROC de ambos modelos utilizando la función `makeRoc` del módulo `plotting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keras  Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
    "print(\"hls4ml Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.makeRoc(y_test, y_keras, le.classes_)\n",
    "plt.gca().set_prop_cycle(None)  # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_hls, le.classes_, linestyle='--')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "lines = [Line2D([0], [0], ls='-'), Line2D([0], [0], ls='--')]\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "leg = Legend(ax, lines, labels=['keras', 'hls4ml'], loc='lower right', frameon=False)\n",
    "ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintesis\n",
    "Ahora vamos a utilizar Vivado HLS para sintetizar el modelo. Podemos ejecutar la construcción utilizando un método de nuestro objeto hls_model. Después de ejecutar este paso, podemos integrar el IP generado en un flujo de trabajo para compilarlo para una placa FPGA específica. En este caso, simplemente revisaremos los informes que genera Vivado HLS, verificando la latencia y el uso de recursos.\n",
    "\n",
    "**Esto puede llevar varios minutos.**\n",
    "\n",
    "Mientras la Síntesis en C se está ejecutando, podemos monitorear el progreso observando el archivo de registro, abriendo una terminal desde el directorio de inicio del cuaderno y ejecutando:\n",
    "\n",
    "`tail -f model_1/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar los informes\n",
    "Imprime los informes generados por Vivado HLS. Presta atención a las secciones de Latencia y 'Estimaciones de utilización'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "Dado que `ReuseFactor = 1`, esperamos que cada multiplicación utilizada en la inferencia de nuestra red neuronal use 1 DSP. ¿Es esto lo que observamos? (Ten en cuenta que la capa Softmax debería usar 5 DSP, o 1 por clase).\n",
    "Calcula cuántas multiplicaciones se realizan para la inferencia de esta red...\n",
    "(Discutiremos el resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
